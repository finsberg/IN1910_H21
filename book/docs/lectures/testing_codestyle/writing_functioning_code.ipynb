{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing Reliable Software\n",
    "\n",
    "This week we will start to look closer at *how* to write reliable and readable software. How should we develop code that is trustworthy? In this lecture we look closer at assertions, exceptions and testing, and in the next lecture we look at code style and documenting code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### What constitutes *reliable* software?\n",
    "\n",
    "To say that software is reliable means that we should be able to trust it. Ideally speaking, this means the program should produce answers that are correct. However, how can we trust that the answers are correct? And what should happen if we meet edge cases were we can no longer be sure that the output will be correct? To explain what we mean when we say reliable, let us list some important points:\n",
    "* Software should not return incorrect results\n",
    "* Software should fail rather than return incorrect results\n",
    "* Software should fail in controlled ways\n",
    "* Software should be tested solidly\n",
    "\n",
    "So ideally we want our code to produce correct results, but we would much rather that our program *fail* than it return incorrect results. And when it fail, it should fail in *controlled ways*, i.e., it should ideally give some nice error message we can use to understand what went wrong. A program shouldn't corrupt any data files or similar if it has to halt half-way through running, and so on. Lastly, the only way to trust our software is to *test* it. It doesn't matter how competent a programmer, there will always be bugs, and testing can help us track these down. \n",
    "\n",
    "Let us look closer at achiveing these elements of reliable software in Python. First we cover how to write programs that fail on purpose in a controlled manner, and then we turn to testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assertions\n",
    "\n",
    "The first thing we shall talk about are *assertions*. Most programming language support writing assertions in a simple manner. Assertions are meant to be very simple \"emergency stops\" in our code and can be a nice and simple way to \"idiot proofing\" your code. The idiot in this case can be other users of your code, but also your future self.\n",
    "\n",
    "In Python an assertion is written using the `assertion` keyword, followed by a condition that is evaluated to either true or false, much like an if-test:\n",
    "```\n",
    "assert <condition>\n",
    "```\n",
    "If the condition is true, the assertion does nothing and the code continues. If it is false, the program crashes with an `AssertionError`. In this sense, an assertion is a check that checks if some requirement is fulfilled, if it isn't, the execution of the program is stopped.\n",
    "\n",
    "In addition to the condition itself, we can put in a string that is given to the user in case of failure:\n",
    "```\n",
    "assert <condition>, \"fail message\"\n",
    "```\n",
    "It is useful to add a simple message, as it makes it easier to understand what went wrong and fix it.\n",
    "\n",
    "Let us look at a specific example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sphere:\n",
    "    def __init__(self, radius):\n",
    "        assert radius >= 0, \"Radius cannot be negative.\"\n",
    "        self.radius = radius"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are defining there `Sphere`-class that we discussed in last week's lecture. The constructor takes the radius of the `Sphere` as an argument, and the class can have various functionality added. Now, we know that a negative radius doesn't make any sense, but Python doesn't know this. So it is completely possible to add a negative radius. We therefore add an `assert` to check the input.\n",
    "\n",
    "If we now try to define a sphere as normal, it works fine, but if we try to do something like:\n",
    "```Python\n",
    "Sphere(-4)\n",
    "```\n",
    "we get the following error\n",
    "```\n",
    "AssertionError: Radius cannot be negative.\n",
    "```\n",
    "Adding the assertion takes us about 10 seconds. But it can potentially save us, or others, a lot of frustration later, as it might catch a bug where someone misunderstands how our class works, or perhaps have a different bug where they send the wrong variable in as an argument.\n",
    "\n",
    "Assertions can also be put into programs to check for \"can't happen\" scenarios. To understand where it makes sense to put assertions, think of how you yourself look for a bug in a program that is misbehaving. You probably put in various print statements to check inputs, outputs, types, and so forth. When you do this, an alternative would be to put in `asserts` checking what you expect to be the case at logical locations. If these assertions fail, you have probably found your bug. The benefit of putting in asserts is that your program will now also catch bugs that might arise later.\n",
    "\n",
    "Assertions are closely linked to something called programming <a href=\"https://en.wikipedia.org/wiki/Invariant_(computer_science)\">invariants</a> in computer science. The *invariants* of a program, solution or algorithm are statements the programmer can rely on being true. Invariants are important when doing more formal software development, as it is a concrete way to making sure correct behavior can be checked or enforced, which can be important for future use and reliableness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disabling Assertions\n",
    "\n",
    "Assertions are nice because they are fast to write, but also efficient to run. Having assertions in your code most likely won't cause any significant slow-down. However, in Python we can also run in and \"optimized\" mode were assertions are ignored. Say an assertion for example occurs inside a function that is called repeatedly in a large loop or something, then it might be nice to run without checking the assertions. This can be done by running Python with the `-O` flag:\n",
    "```Python\n",
    "python -O run_simulation.py\n",
    "```\n",
    "This flag only does two things: \n",
    "- completely ignores any assertion\n",
    "- sets the builtin variable `__debug__` to false (it is True by default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exceptions\n",
    "\n",
    "Assertions were used to abort the program given that some condition was not met. However, there is a different way to halt the execution of a program in a controlled manner, raising exceptions.\n",
    "\n",
    "We raise an exception by using the `raise` keyword as follows\n",
    "```\n",
    "raise <exception>\n",
    "```\n",
    "where we fill the proper exception type. All the normal \"errors\" you get in Python (`TypeError`, `NameError`, etc) are exceptions. So in our Sphere example as before, we could change our assertion to raising an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sphere:\n",
    "    def __init__(self, radius):\n",
    "        if radius < 0:\n",
    "            raise ValueError(\"Radius cannot be negative\")\n",
    "        self.radius = radius"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we put the error message as a string into the constructor of the exception (because the exceptions are classes in Python). This time, if we try to define `Sphere(-4)` the program crashes, but with the error message\n",
    "```\n",
    "ValueError: Radius cannot be negative.\n",
    "```\n",
    "One apparent benefit of this over the assertion is that we get more info out. The `ValueError` tells us its something with the *value* that is going wrong. This type is also very important when we *catch exceptions*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catching Exceptions\n",
    "\n",
    "When a exception is raised, for example during a function call, it will crash the program, unless that exception is *caught*. In Python, we catch exceptions by putting them inside a `try`-block. Let us for example we have a long list of values, and we want to create a corresponding list of Spheres, with radii given by those values. However, the values are sometimes negative, in which case we want the radii to be set to 0. We could then do the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sphere(4.5), Sphere(0), Sphere(3.4), Sphere(2.7), Sphere(0), Sphere(6.3)]\n"
     ]
    }
   ],
   "source": [
    "class Sphere(Sphere):\n",
    "    def __repr__(self):\n",
    "        return(self.__class__.__name__ + f\"({self.radius})\")\n",
    "\n",
    "data = [4.5, -1.1, 3.4, 2.7, -0.2, 6.3]\n",
    "spheres = []\n",
    "\n",
    "for d in data:\n",
    "    try:\n",
    "        spheres.append(Sphere(d))\n",
    "    except ValueError:\n",
    "        spheres.append(Sphere(0))\n",
    "        \n",
    "print(spheres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Try`/`Except` structure has a fairly intuitive names in Python. First we *try* something, if something specific goes wrong, we do something else. We could for example *try* opening a file, but if there is no file, we could instead make one, or choose to use some default data, or ask the user for a new file. In most languages it is also called \"try\", but instead of \"except\" we call it \"catch\". This is because *raising* an error is also referred to *throwing* an error, and we can then throw an error somewhere in the code, for example inside a function, and then catch it and handle it somewhere else.\n",
    "\n",
    "Note that we specifically catch a `ValueError`, because we know this is what can go wrong. It is possible to just write `except:`, and this catches *any* exception. However, this is not recommended, because this would also catch a simple syntax error inside the Sphere class for exampleâ€”which should crash the program. Because we catch a `ValueError` specifically, we let all other erros still go through to the user as desired.\n",
    "\n",
    "You can also catch errors of different types and handle them similarily as follows:\n",
    "```Python\n",
    "try:\n",
    "    ...\n",
    "except (ValueError, TypeError):\n",
    "    ...\n",
    "```\n",
    "or you can have several, different except blocks:\n",
    "```Python\n",
    "try:\n",
    "   ...\n",
    "except ValueError:\n",
    "   ...\n",
    "except TypeError:\n",
    "   ...\n",
    "```\n",
    "\n",
    "You can also get the error message of the error if you write \n",
    "```\n",
    "except ValueError as e:\n",
    "```\n",
    "in which case `e` will be a string with the error message.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error types in Python\n",
    "\n",
    "In Python, there is a general superclass for all errors called simply `Exception`. All other errors inherit from this class. The following diagram shows all the built in errors\n",
    "\n",
    "```\n",
    "+-- Exception\n",
    "      +-- StandardError\n",
    "      |    +-- ArithmeticError\n",
    "      |    |    +-- FloatingPointError\n",
    "      |    |    +-- OverflowError\n",
    "      |    |    +-- ZeroDivisionError\n",
    "      |    +-- AssertionError\n",
    "      |    +-- AttributeError\n",
    "      |    +-- EnvironmentError\n",
    "      |    |    +-- IOError\n",
    "      |    +-- EOFError\n",
    "      |    +-- ImportError\n",
    "      |    +-- LookupError\n",
    "      |    |    +-- IndexError\n",
    "      |    |    +-- KeyError\n",
    "      |    +-- NameError\n",
    "      |    +-- RuntimeError\n",
    "      |    |    +-- NotImplementedError\n",
    "      |    +-- SyntaxError\n",
    "      |    |    +-- IndentationError\n",
    "      |    |         +-- TabError\n",
    "      |    +-- SystemError\n",
    "      |    +-- TypeError\n",
    "      |    +-- ValueError\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any of these can be raised and caught in your code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Defining Custom Exceptions\n",
    "\n",
    "There are many built in exceptions and errors that can be used to make your errors more specific. However, it is also trivial to define *custom* exceptions in your code. Simply subclass an existing exception class, or `Exception` itself.\n",
    "\n",
    "Why would you want to define your own custom exceptions? For larger projects these can be nice, as they can make it even clearer what is going wrong, or they can make sure that you are catching exactly the bug you are expecting. Say for example our earlier Sphere example, where the radius shouldn't be negative. We could raise a ValueError, but we could also create a custom exception that can be even more specific."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeRadiusError(Exception):\n",
    "    pass\n",
    "\n",
    "class Sphere:\n",
    "    def __init__(self, radius):\n",
    "        if radius < 0:\n",
    "            raise NegativeRadiusError(\"Radius must be non-negative\")\n",
    "        self.radius = radius\n",
    "    \n",
    "    def shrink(self, shrinkage):\n",
    "        if self.radius - shrinkage < 0:\n",
    "            raise NegativeRadiusError(\"Radius would become negative.\")\n",
    "        self.radius -= shrinkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Radius would become negative.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    s = Sphere(2)\n",
    "    s.shrink(3)\n",
    "except NegativeRadiusError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we subclass the `Exception` class with a new custom exception called `NegativeRadiusError`. We just write `pass` because we are not overwriting any functionality from the exception class, we are just defining a new class with a custom name. Now we are free to raise and catch this error in the rest of our code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Assertions vs other Exceptions\n",
    "\n",
    "We have now covered how we can use both assert statements, and raising exceptions to get a program to fail controllably. But for which cases should you use asserts and when should you use exceptions? The main benefit of asserts are that they are very fast to implement, and that they can be turned of to run in \"optimized\" mode. The main benefit of raising exceptions is that we can be more specific with the exception type, which can then be easier to catch and handle in other parts of the program.\n",
    "\n",
    "Because of their benefits, asserts should be used to test for conditions that should never happen. We use them mostly as an aid when developing the code. For any error that we expect to possibly happen during normal use of the software should instead raise exceptions, which will be much better for the end user due to the better information and handling possibilities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "So far we have talked about how to get the program to fail in a controlled manner. Now we turn to how to *test* code. When writing larger pieces of software, we will make mistakes, 100 % guaranteed. Tests let us catch these errors before the software is used for something important.\n",
    "\n",
    "Testing code means checking if it is behaving as expected, and that it meets our requirements. [Software testing](https://en.wikipedia.org/wiki/Software_testing) is a large and broad field, and its possible to work as a dedicated software tester. In IN1910 we won't go too in depth in testing, but show you some nice tools to test your own code. Testing is an essential part of quality assurance, and for software to be considered high quality it has to include testing. Others won't trust your code if it isn't tested, and neither should you.\n",
    "\n",
    "One of the simplest forms of testing, and the one we will focus most on in IN1910, is called *unit testing*.\n",
    "\n",
    "### Unit Testing\n",
    "\n",
    "A unit test is a test of a small component of a program, a single \"unit\". If we are talking about object-oriented programming, a unit test would for example check that a single method of a class does what it is expected to do. When unit testing a piece of code, we are checking that piece of code is working in isolation. The unit test offers no insurance the the code as a whole works, but at least we can feel more confident in that piece of the code. While this might sound like unit tests are *too* simple, it also makes them straight-forward to think up and implement. When starting a new project, you can write a piece of code, then make the unit tests for that piece of code right there and then. Verifying that the the whole code works on the other hand, won't be possible untill you've written the whole code.\n",
    "\n",
    "Another important point about unit tests is that they should be *automated*, meaning that it should be a a test case that can run by itself, and also check wether it passes or fails by itself. This way, we write the unit test once, and then run it every time we change the code to make sure nothing is broken. The goal of automated testing is of course efficiency, but also that we don't get sloppy over time. To automate the process of making and running tests, we want to use a package meant for unit testing.\n",
    "\n",
    "### The pytest package\n",
    "\n",
    "As with everything else in software, many packages exist for unit testing in Python. In fact, Python has its own built in package called `unittest` which we could use. Other popular choices are `pytest`, `doctest` and `nose`. These all have slightly different syntax, workflow, and use cases, but in most regards they are quite similar. In IN1910, we only write fairly simple tests, and all of these frameworks would work well. We opt to go for `pytest` because it by far the most used testing library in python, it is easy to use but also has a lot of powerful extensions if you want to use more advaced testing features.\n",
    "\n",
    "Note that while we use `pytest` to write our unit tests, the concept of unit testing doesn't rely on any specific framework, the important takeaways are the ideas.\n",
    "\n",
    "We use the rest of this lecture to show example of how to write unit tests in `pytest`. If you want a more thorough introduction to `pytest`, the following tutorials might be helpful\n",
    "* [pytest introduction by Brian Okken](http://pythontesting.net/framework/pytest/pytest-introduction/)\n",
    "* [official pytest documentation](https://docs.pytest.org/en/latest/)\n",
    "\n",
    "Before you continue, you should make sure that you have `pytest` installed. `pytest` is not part of the standard library, but can easily be installed with e.g `pip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytest in /usr/local/lib/python3.7/site-packages (3.7.2)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/site-packages (from pytest) (1.1.5)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/site-packages (from pytest) (4.3.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/site-packages (from pytest) (1.11.0)\n",
      "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/site-packages (from pytest) (1.5.4)\n",
      "Requirement already satisfied: pluggy>=0.7 in /usr/local/lib/python3.7/site-packages (from pytest) (0.7.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/site-packages (from pytest) (40.8.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/site-packages (from pytest) (18.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pytest basics\n",
    "\n",
    "As we said earlier, the main goal of unit tests is to automate things, `pytest` therefore finds our tests automatically. It does this by assuming everything (files, functions, classes) with a name begining with `test` or `Test` is a test. When we ask pytest to run tests it looks for these tests and runs all of them, and giving us a report back about which ones succeeded and which ones failed. Any test that finishes executing without throwing an exception *succeeds*, any test that throws an exception fails.\n",
    "\n",
    "It is very useful to put all your tests into one or several files with names starting with `test_`. By keeping tests seperate from your source code, you keep everything nice and tidy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unit testing examples with `Vector3D`\n",
    "\n",
    "As a running example of unit testing, we will use the `Vector3D` class we made during the previous lecture. In this case we have our source code in the file `vector.py`. And we would put all our tests in a separate file called `test_vector.py`. In these lecture notes we simply import the `Vector3D` class and test it directly to show examples. But we also put the `test_vector.py` file there so you can try running pytest yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's stary writing some unit tests for `Vector3D`. To do this we first have to think of examples of how the code *should* behave. Let us for example define to vectors\n",
    "* $u = (1,2,0)$\n",
    "* $v = (1, -1, 3)$\n",
    "\n",
    "And then we can compute by hand:\n",
    "* $u+v = (2, 1, 3)$\n",
    "* $u-v = (0, 3, -3)$\n",
    "* $u\\cdot v = -1$\n",
    "* $||u||^2 = 5$\n",
    "* $||v||^2 = 11$\n",
    "\n",
    "So we know what these simple computations *should* be. What we do now is implement a unit test to check if this is actually the case for our code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we want to check that `u + v` gives us the expected values. To check this we use assertions. We can write this test out as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vector import Vector3D\n",
    "def test_add():\n",
    "    u = Vector3D(1, 2, 0)\n",
    "    v = Vector3D(1, -1, 3)\n",
    "    w = u + v\n",
    "    assert w.x == 2\n",
    "    assert w.y == 1\n",
    "    assert w.z == 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this test we create two vectors and add them together like we would when using the class normally. Then we check the result using `assert w.z == 3`. If these pass, meaning the results are as expected, the test passes (we don't need to return true or anything like that). If they fail, the test fails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this function's name starts with `test_`, it will automatically be found an run by pytest. You can run `pytest` from the command line by writing \n",
    "```\n",
    "pytest\n",
    "```\n",
    "or\n",
    "```\n",
    "py.test\n",
    "```\n",
    "Another way to run `pytest` is to execute it as a module\n",
    "```\n",
    "python3 -m pytest\n",
    "```\n",
    "This is very handy if you have different versions of python installed and want to be certain that you run pytest with the version that you want. \n",
    "In which case it looks for all tests in the current folder and any subfolders. Or you could run it on a specific folder by writing\n",
    "```\n",
    "pytest <folder>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we run pytest now, we get the following ouput\n",
    "```\n",
    "collected 1 item\n",
    "\n",
    "test_vector.py .                                                                                                 [100%]\n",
    "\n",
    "============================================== 1 passed in 0.14 seconds ===============================================\n",
    "```\n",
    "It ran 1 test, because that is all we have written so far. It says the test passed, which means there were no exceptions or assertion errors. This means our *add* function works as expected, at least for this input! \n",
    "\n",
    "Let us produce an error on purpose to see how that looks. We go and change the `Vector3D.__add__` method so that we do something wrong on purpose, say for example:\n",
    "```\n",
    "x = self.x + other.x\n",
    "y = self.x + other.x\n",
    "z = self.z + other.z\n",
    "```\n",
    "(Copying and pasting code that needs small adjustments is an extremely common place for errors). We would get the following result from pytest:\n",
    "```\n",
    "collected 1 item\n",
    "\n",
    "test_vector.py F                                                                                                 [100%]\n",
    "\n",
    "====================================================== FAILURES =======================================================\n",
    "______________________________________________________ test_add _______________________________________________________\n",
    "\n",
    "    def test_add():\n",
    "        u = Vector3D(1, 2, 0)\n",
    "        v = Vector3D(1, -1, 3)\n",
    "        w = u + v\n",
    "        assert w.x == 2\n",
    ">       assert w.y == 1\n",
    "E       assert 2 == 1\n",
    "E        +  where 2 = Vector3D(2, 2, 3).y\n",
    "\n",
    "test_vector.py:9: AssertionError\n",
    "============================================== 1 failed in 0.19 seconds ===============================================\n",
    "```\n",
    "The test fails because `w.y` would become 2, because of the bug, but we assert it to be 1. Note that the error message points to which line the error occurs(`>assert w.y == 1`), and explains what the wrongfully assertion is(`2 == 1`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statement `assert` effectively asserts that two variables are equal, i.e., that `a == b` is true. In this case, we checked each component seperately. However, we could also try `assert w == Vector3D(2, 1, 3)`, but this would fail. This is because we are comparing custom objects, and when comparing custom objects, Python defaults to comparing their location in memory. Because we are comparing two different objects, this would therefore fail by default.\n",
    "\n",
    "To make vectors comparable by their values, we need to add a `__eq__` special method to our `Vector3D` method, when we check for equality by doing `u == v`, this is equivalent to calling `u.__eq__(v)`, so our method should return a True or a False.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vector3D(Vector3D):\n",
    "    def __eq__(self, other):\n",
    "        same_x = abs(self.x - other.x) < 1e-12\n",
    "        same_y =abs(self.y, other.y) < 1e-12\n",
    "        same_z = abs(self.z, other.z) < 1e-12\n",
    "        return same_x and same_y and same_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In stead of asserting that the numbers are equal, we instead check that the two numbers are close, by checking that the absolute value of their difference is less that some tolerance (here $10^{-12}$).\n",
    "\n",
    "When testing for equality of floating point numbers, you should allways check that numbers are close in stead of strict equality. The reason is that floating point operations can lead to round off errors which can be seen in the following example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19999999999999996\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-8ca4158e488d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "a = 1.2\n",
    "b = 1.0\n",
    "print(a - b)\n",
    "assert a - b == 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the equality method implemented, let's write a unit test to check if it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_eq():\n",
    "    u = Vector3D(1, 2, 0)\n",
    "    assert u == Vector3D(1, 2, 0)\n",
    "    assert u == Vector3D(1.00000001, 2, 0)\n",
    "    assert u != Vector3D(1, 2, 1)\n",
    "    assert u != Vector3D(1.001, 2, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First note that we need to define a new vector to test with, because each unit test we write is a seperate function, they don't see each other. Sometimes stated as: *\"Each unit test is an island.\"*. We write four assertions:\n",
    "1. two vectors of the same components should be equal\n",
    "2. two vectors with a small round-off error should also be equal\n",
    "3. two vectors with a different component should not be equal\n",
    "4. two vectors with a small, but noticable difference, should not be equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the tests with pytest show that things are working as expected. With the `__eq__` implemented and tested we could now write out the add test as follows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_add():\n",
    "    u = Vector3D(1, 2, 0)\n",
    "    v = Vector3D(1, -1, 3)\n",
    "    assert (u + v) == Vector3D(2, 1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us move on and make a few more tests for the following cases:\n",
    "* $u+v = (2, 1, 3)$\n",
    "* $u-v = (0, 3, -3)$\n",
    "* $u\\cdot v = -1$\n",
    "* $u\\times v = (6, -3, -3)$\n",
    "* $(u\\times v)\\cdot u = 0$ and $(u\\times v)\\cdot v = 0$\n",
    "* $||u||^2 = 5$\n",
    "* $||v||^2 = 11$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sub():\n",
    "    \"\"\"Test subtraction\"\"\"\n",
    "    u = Vector3D(1, 2, 0)\n",
    "    v = Vector3D(1, -1, 3)\n",
    "    assert (u - v) == Vector3D(0, 3, -3)\n",
    "    \n",
    "def test_dot():\n",
    "    \"\"\"Test dot product\"\"\"\n",
    "    u = Vector3D(1, 2, 0)\n",
    "    v = Vector3D(1, -1, 3)\n",
    "    assert u.dot(v) == -1\n",
    "    assert u*v == -1\n",
    "    \n",
    "def test_cross():\n",
    "    \"\"\"Test cross product\"\"\"\n",
    "    u = Vector3D(1, 2, 0)\n",
    "    v = Vector3D(1, -1, 3)\n",
    "    assert u.cross(v) == Vector3D(6, -3, -3)\n",
    "    assert u@v == Vector3D(6, -3, -3)\n",
    "\n",
    "def test_perp():\n",
    "    \"\"\"Test perpendicularity\"\"\"\n",
    "    u = Vector3D(1, 2, 0)\n",
    "    v = Vector3D(1, -1, 3)\n",
    "    assert not u.perpendicular(v)\n",
    "    assert (u@v).perpendicular(u)\n",
    "    assert (u@v).perpendicular(v)\n",
    "\n",
    "def test_length():\n",
    "    \"\"\"Test length\"\"\"\n",
    "    u = Vector3D(1, 2, 0)\n",
    "    v = Vector3D(1, -1, 3)\n",
    "    assert (u.length**2 - 5) < 1e-12\n",
    "    assert (v.length**2 - 11)< 1e-12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run our tests are before, and we will get the following output\n",
    "```\n",
    "collected 7 items\n",
    "\n",
    "test_vector.py .......                                                                                           [100%]\n",
    "\n",
    "============================================== 7 passed in 0.05 seconds ===============================================\n",
    "```\n",
    "Every test passed! Not very surprising as we had already tested our code somewhat with simple examples during the previous lecture. But this is a lot more formal and rigorous, and shows anyone who missed that lecture that it seems to be working.\n",
    "\n",
    "To get more feedback on what tests are run, we can add the `-v` flag:\n",
    "```\n",
    "pytest -v\n",
    "```\n",
    "to get the output:\n",
    "```\n",
    "collected 7 items\n",
    "\n",
    "test_vector.py::test_add PASSED                                                                                  [ 14%]\n",
    "test_vector.py::test_eq PASSED                                                                                   [ 28%]\n",
    "test_vector.py::test_sub PASSED                                                                                  [ 42%]\n",
    "test_vector.py::test_dot PASSED                                                                                  [ 57%]\n",
    "test_vector.py::test_cross PASSED                                                                                [ 71%]\n",
    "test_vector.py::test_perp PASSED                                                                                 [ 85%]\n",
    "test_vector.py::test_length PASSED                                                                               [100%]\n",
    "\n",
    "============================================== 7 passed in 0.11 seconds ===============================================\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Asserting exception raising\n",
    "\n",
    "One last thing we often want to check for, is that an exception is actually raised given the right conditions. This can easily be implemented using something called a context manager. When testing for an exception we want to allow a specfic exception to be raised, but we would also like to fail the test if that specfic exception was not raised. \n",
    "To do this we create a context block using a `with` statement as be calling `pytest.raises`, `pytest` will listen for for the exception that we provided within that context block, and fail the test if we exit the block without the specific exeption raised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "def test_no_scalar_addition():\n",
    "    with pytest.raises(TypeError):\n",
    "        Vector3D(1, 1, 0) + 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we check that a vector $u$ plus a scalar $3$ is undefined as a TypeError. Note that here we also need to import `pytest` because we are using a function from the `pytest` library.\n",
    "\n",
    "Let us show a different one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_no_unit_vector():\n",
    "    with pytest.raises(RuntimeError):\n",
    "        Vector3D(0, 0, 0).unit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which verifies that we cannot find a unit-vector for a vector with zero length. When we now run pytest we get the following output (changed somewhat for readability in the notebook)\n",
    "```\n",
    "collected 9 items\n",
    "\n",
    "test_vector.py ........F                                                                                         [100%]\n",
    "\n",
    "====================================================== FAILURES =======================================================\n",
    "_________________________________________________ test_no_unit_vector _________________________________________________\n",
    "\n",
    "    def test_no_unit_vector():\n",
    "        with pytest.raises(RuntimeError):\n",
    ">           Vector3D(0, 0, 0).unit()\n",
    "E           Failed: DID NOT RAISE <class 'RuntimeError'>\n",
    "\n",
    "test_vector.py:79: Failed\n",
    "```\n",
    "This means the last test failed, but the other 8 passed. The last test fails because we never added functionality that raises an exception for the zero-length vector case. Let us add this now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vector3D(Vector3D):\n",
    "    def unit(self):\n",
    "        if self.length == 0:\n",
    "            raise RuntimeError(\"Vector of zero length has no unit vector.\")\n",
    "        \n",
    "        new_vector = Vector3D(self.x, self.y, self.z)\n",
    "        new_vector.length = 1\n",
    "        return new_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And after adding this, our test passes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the test-module\n",
    "\n",
    "So far we have used the command-line tool `pytest` to automatically find and run all our tests. Running our test-modules directly through Python however, i.e,\n",
    "```Bash\n",
    "python test_vector.py\n",
    "```\n",
    "Produces no output. This is because our test-module only defines a bunch of functions, and never *calls* them. If we also want to be able to run our test script directly, we can add a main-block at the bottom of our program, where we can call `pytest.main()`\n",
    "```Python\n",
    "if __name__ == '__main__':\n",
    " \tpytest.main()\n",
    "```\n",
    "The `pytest.main()` command automatically runs all the tests in the module. This way we can run all our tests either by calling `pytest` in the command-line, or running the test module through our editor or similar. Be sure to put the `pytest.main()` command in a main-block, otherwise it would be called every time the module is imported."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now covered what we want to cover on making and running unit tests in *pytest*. For the remainder of this lecture we will cover some more terminology and theory on testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integration testing\n",
    "\n",
    "We have mostly talked about testing in the terms of unit testing, but this is obviously not the only kind of testing out there. On the other end of the spectrum we find *integration testing*, which is where we test a larger codebase to find out how the different components of the code base work together and *integrate* into a complete program or software package (The term isn't directly connected to calculus). For scientific applications especially, this is quite important, because for larger mathematical modeling, comparing your output to analytical solutions or similar means we need to actually write an example use-case, which would be an example of an integration test.\n",
    "\n",
    "An analogy could be to think of how a car is made. First, individual components of the car is made in different factories spread across several countries or even continets. Some make the wheels, some the axels, some the engine, etc. Each individual factory obviously need to test the components they are producing and shipping out. These \"component tests\" would be the unit tests. Finally, we have the factory that takes all the different components and builds the cars (the components are \"integrated\" into a car). When the car is finished, it too should be tested before being sold, which would be the integration tests.\n",
    "\n",
    "As seen in the analogy, unit testing code and integration testing aren't competitors, and good software needs both. However, the two are done in different parts of the development. The unit tests are written before and during development. They need to test each component *as they are made*. Integration tests however, are carried out once you have nearly finished code. In IN1910 we focus on writing automated unit tests with `pytest`, and then finding some good example case where we can compare the output to a known analytic solution as our integration case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression testing\n",
    "\n",
    "A different kind of term you might come across is called \"regression testing\". This isn't really a third type of test, but is more descriptive of the *goal* of the testing. A \"regression\" in this sense means a step back, and the goal of regression tests are to make sure that code that has been implemented and works, isn't changed and broken. In this sense, unit tests can be a type of regression test. Say you have a well-tested piece of code with plenty of unit tests. Now you want to refactor, change or optimize that code in some way. Then you can rerun all your unit tests after incorporating your changes, and thus check for regressions.\n",
    "\n",
    "In larger software projects, regression tests are also used to fix bugs. When a bug or issue is found, the first step is often to find out how to reproduce that bug reliably. Here the developers often ask for a *minimal working example*, i.e., the least amount of code needed to cause the bug to happen. This minimal working example can be made into a test. This is extremely useful in the long run because this will become an automatic regression test. When the bug is fixed, the newly created test will pass. If the bug is ever reintroduced later, the regression test will imediately catch it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Agile software development\n",
    "\n",
    "[Agile software development](https://en.wikipedia.org/wiki/Agile_software_development) is a modern (2001-) set of software development methods that is seeing frequent use in the industry. It focuses tools, methods and structures that make it easier to develop software quickly and efficiently, while maintaing quality and flexibility.\n",
    "\n",
    "In agile software development, there is a strong focus on testing, and especially on [test-driven development](https://en.wikipedia.org/wiki/Test-driven_development). Test driven development means that all development is focused around testing, rather than doing testing as an afterthought. In fact, ideally, one writes the tests *first*, and then you develop code untill that test passes. This way, it is clear when the code is doing what it is supposed to do. In this way, the tests can be seen also as a code specification.\n",
    "\n",
    "One of the main ideas of test-driven development can be summarized by:\n",
    "> Write tests first, otherwise you'll write them never\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing and Version Control\n",
    "\n",
    "As mentioned earlier, for testing to be as efficient as possible, we want to automate testing. This ideally also means that we shouldn't need to run the tests ourselves, rather it should happen automatically every time we change the code. For a software project, the code changes when someone *commits and pushes* new code. Because of this, it should come as no surprise that there are many tools possible to automate the running of all tests in a git repository any time that code is changed.\n",
    "\n",
    "Tools that do this are often known as often known as [continuous integration](https://en.wikipedia.org/wiki/Continuous_integration) tools, and these \"watch\" our repository and lets us know when something seems to break. CI tools can also do other kinds of testing, such as \"building\" where it sets up a clean virtual machine, installs all the dependencies of your code, installs your code, and then runs all the tests. This way, the CI watcher can also catch when your code breaks not because *you* introduced some bug, but because some other package is changed somehow. CI can automate such build-tests at regular intervals, perhaps several times a day.\n",
    "\n",
    "The most popular CI tool for Github is [Travis CI](https://travis-ci.org/), but many other options exist, such as [Jenkins](http://jenkins-ci.org) and [Bitbucket Bamboo](https://www.atlassian.com/software/bamboo). \n",
    "\n",
    "Setting up CI monitoring is recommended if you ever write software you want others to start using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test coverage\n",
    "\n",
    "Finally. If testing is a cruical part of good software, how can we measure how *well* a software is tested? Having some tests is better than having no tests. But is having many tests a guarantee of good code? The short answer is: no, it isn't. One metric of measuring how well software is tested is called test *coverage*. It is a simple measurement of how much of the code is actually run when running the tests. \n",
    "\n",
    "There are different ways to measure \"coverage\", but the simplest way is usually \"line coverage\", which simply denotes the percentage of lines that are actually run when going through all the tests. If we have a line coverage of 30%, then the unit tests only use 30% of the actual code, and so 70% of the code is never actually tested.\n",
    "\n",
    "`pytest` comes with many [plugins](https://pytest.readthedocs.io/en/2.7.3/plugins_index/index.html), and one of them is `pytest-cov` which can be used to check coverage for us when we run our unit tests. To use it you first need to install the plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytest-cov in /Users/henriknf/miniconda3/lib/python3.7/site-packages (2.7.1)\n",
      "Requirement already satisfied: coverage>=4.4 in /Users/henriknf/miniconda3/lib/python3.7/site-packages (from pytest-cov) (4.5.3)\n",
      "Requirement already satisfied: pytest>=3.6 in /Users/henriknf/miniconda3/lib/python3.7/site-packages (from pytest-cov) (4.5.0)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in /Users/henriknf/miniconda3/lib/python3.7/site-packages (from pytest>=3.6->pytest-cov) (1.3.0)\n",
      "Requirement already satisfied: more-itertools>=4.0.0; python_version > \"2.7\" in /Users/henriknf/miniconda3/lib/python3.7/site-packages (from pytest>=3.6->pytest-cov) (7.0.0)\n",
      "Requirement already satisfied: pluggy!=0.10,<1.0,>=0.9 in /Users/henriknf/miniconda3/lib/python3.7/site-packages (from pytest>=3.6->pytest-cov) (0.11.0)\n",
      "Requirement already satisfied: setuptools in /Users/henriknf/miniconda3/lib/python3.7/site-packages (from pytest>=3.6->pytest-cov) (40.6.3)\n",
      "Requirement already satisfied: py>=1.5.0 in /Users/henriknf/miniconda3/lib/python3.7/site-packages (from pytest>=3.6->pytest-cov) (1.8.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /Users/henriknf/miniconda3/lib/python3.7/site-packages (from pytest>=3.6->pytest-cov) (19.1.0)\n",
      "Requirement already satisfied: wcwidth in /Users/henriknf/miniconda3/lib/python3.7/site-packages (from pytest>=3.6->pytest-cov) (0.1.7)\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/henriknf/miniconda3/lib/python3.7/site-packages (from pytest>=3.6->pytest-cov) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pytest-cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have the plugin, you can do\n",
    "```\n",
    "pytest --cov\n",
    "```\n",
    "And you will get an output that shows how our line coverage is. It automatically reports on any file that is invoked in the tests, which means it might spew out tons of info on packages such as numpy and scipy. To avoid this we can do\n",
    "```\n",
    "pytest --cov=vector\n",
    "```\n",
    "which tells `pytest` to only report coverage from the package \"vector\", which in this case means the `vector.py` file, but it could be a folder as well. Running our command, we now get the output\n",
    "```\n",
    ".........\n",
    "Name        Stmts   Miss  Cover\n",
    "-------------------------------\n",
    "vector.py      60     14    77%\n",
    "----------------------------------------------------------------------\n",
    "Ran 9 tests in 0.070s\n",
    "\n",
    "OK\n",
    "```\n",
    "Here \"Stmts\" are the number of statements, i.e., code lines, in the vector.py module, \"Miss\" are the number of lines that are *not* covered, and \"Cover\" shows the line coverage as a percentage. In this case we have 77% coverage, not too shabby.\n",
    "\n",
    "There are many other tools for checking line coverage, and some editors and IDE's lets you do it straight in the editor. CI tools can also automaticly check and show coverage on your repository as a HTML \"badge\". See for example the [requests repostiory](https://requests.readthedocs.io/en/master/).\n",
    "\n",
    "Note that coverage is a much used and refered to because it is easy to measure and can be done automatically. However, it is far from perfect. A code with 100% test coverage is not guaranteed to be error-free, and a code with 30% coverage can be better tested than one with 70%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avoiding Confirmation Bias\n",
    "\n",
    "Confirmation bias is the tendency of trying to verify your own hypotheses, rather than trying to *refute* them. The same tendency can be found in software developers, that they tend to write tests that prove their code works, rather than write tests that get the code to fail.\n",
    "\n",
    "It is important to remember the goal of testing is to find errors in the code and fix them. When writing tests, you should therefore try to avoid confirmation bias and write tests that are a bit vicious. Put simply, good tests try to break the code.\n",
    "\n",
    "There is a famous logic puzzle that illustrates confirmation bias well, called the *four-card problem*. See if you can solve it.\n",
    "\n",
    "**Problem:** Suppose you have a deck of cards, where each card has a letter on one side, and a number on the other. On the table in front of you, there are 4 cards, shown below. We now propose the following hypothesis: *\"If a card has a vowel on one side, then the other side is an even number.\"*. Which cards are worthwile to flip over to test this hypothesis?\n",
    "\n",
    "<img src=\"fig/four_card_problem.png\" width=400>\n",
    "\n",
    "\n",
    "**Solution:** This problem was originally formulated by psychologist Peter Cathcart Wason in 1966, who proceeded to test university students with it. They found that \"A and 4\" was the most common answer (46%), followed by \"Just A\" (32%). Both of these answers are wrong. Because A is a vowel, turning this card over is useful, because if the card on the other side is odd, then we have disproved the hypothesis. Turning over the 4 however, isn't helpful, because the state hypothesis is \"one-way\", i.e., it states \"if vowel, then even number\", not vice versa. However, we *should* flip the 7 over, as this is an odd number, and so if ther is a vowel on the other side, then we have disproved the hypothesis! Thus the answer is \"A and 7\".\n",
    "\n",
    "Wason's logic puzzle is a good illustration of confirmation bias. Most people when confronted with the riddle want to try to *prove* the hypothesis, when they should be trying to *disprove* it. The take away to software development is that good tests try to break code, not confirm that it works!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###### Let's end on some humor\n",
    "\n",
    "<img src=\"fig/qa_engineer.png\" width=400>\n",
    "<center>A QA engineer works with \"Quality Assurance\". Put simply, their job is try to break others code.</center>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Linked Lists and Dynamic Arrays, ...and sorting algorithms\n",
    "\n",
    "In this lecture we will start learning what we briefly covered about algorithm analysis and big Oh notation to two different cases. We will be looking at differences between our linked list data structure, and our dynamic array data structure. And we we look closer at some different sorting algorithms, and how they compare in terms of computational complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big Oh\n",
    "\n",
    "Recall from last time, that we introduced the big Oh notation to classify how much *work* an algorithm has to carry out to solve some problem, given the size of that problem. For sorting a list for example, the size of the list would be the problem size. We often denote the problem size by $n$.\n",
    "\n",
    "The main thing we are interested in, is understanding how our algorithm *scales* with the problem size, as the problem grows, does the computational cost grow linearly? Quadratically? Exponentially? This is where the big Oh notation comes in.\n",
    "\n",
    "We find the big Oh of an algorithm, by finding the number of *operations* it has to carry out to solve some problem, but we do not care to find the exact number, we care only about the fastest-growing term. We also disregard any coefficients.\n",
    "\n",
    "Using big Oh notation, we will end up classifying most algorithms into one of the categories in the list below, which is organized from slowest-growing, to fastest-growing. An algorithm that is $\\mathcal{O}(1)$ scales perfectly, we can increase the problem size, and the time to solve it won't change! An algorithm that is $\\mathcal{O(n)}$ scales linearly, if we double the problem size, we will roughly double the computation time. An algorithm that is $\\mathcal{O}(e^n)$ scales horribly, as the problem size grows, the computation time grows exponentially, for larger inputs, the algorithm will stop working as we run out of computational resources.\n",
    "\n",
    "| Big Oh | Name |\n",
    "|--------|------|\n",
    "| $\\mathcal{O}(1)$ | constant | \n",
    "| $\\mathcal{O}(\\log n)$ | logarithmic | \n",
    "| $\\mathcal{O}(n)$ | linear | \n",
    "| $\\mathcal{O}(n \\log n)$ | loglinear/quasilinear | \n",
    "| $\\mathcal{O}(n^2)$ | Quadratic | \n",
    "| $\\mathcal{O}(n^3)$ | Cubic | \n",
    "| $\\mathcal{O}(n^k)$ | Polynomial | \n",
    "| $\\mathcal{O}(e^n)$ | Exponential | \n",
    "| $\\mathcal{O}(n!)$ | Factorial | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing with Big Oh\n",
    "\n",
    "Most algorithms consists of many different steps, which we might analyze independently. The total cost of the algorithm is then the sum of the costs of each step. If we've found the big Oh costs of each step, we then need to add big Oh expressions together. For example:\n",
    "$$\\mathcal{O}(n) + \\mathcal{O}(n^2).$$\n",
    "When adding big Oh expressions together, we simply keep the biggest term:\n",
    "$$\\mathcal{O}(n) + \\mathcal{O}(n^2) = \\mathcal{O}(n^2).$$\n",
    "If there are multiply of the biggest term, this doesn't really matter, as adding these expressions together would just give a different coefficient, but we are disregarding that coefficient. So for example:\n",
    "$$\\mathcal{O}(1) + \\mathcal{n} + \\mathcal{n} = \\mathcal{O}(n).$$\n",
    "\n",
    "Similarily, sometimes we need to carry out an operation many times, for example when iterating or looping. This means multiplying a cost by some number. For example\n",
    "$$10\\mathcal{O}(n^2).$$\n",
    "This would also just be a constant coefficient, and so we would simply discard it:\n",
    "$$10\\mathcal{O}(n^2) = \\mathcal{O}(n^2).$$\n",
    "\n",
    "The exception to this if the number of steps we carry out is itself a function of the problem size $n$. For example iterating a list of size $n$, or performing $n$ multiplications, etc. In that case, we need to multiply in the number of repetitions into the big Oh. Performing $n$ steps that each cost $\\mathcal{O}(n)$ for example: \n",
    "$$n \\cdot \\mathcal{O}(n) = \\mathcal{O}(n^2).$$\n",
    "\n",
    "Or performing $n$ steps of an algorithm costing $\\mathcal{O}(n\\log n)$ would give a total cost of:\n",
    "$$n\\cdot \\mathcal{O}(n\\log n) = \\mathcal{O}(n^2 \\log n).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The List ADT\n",
    "\n",
    "In the previous lectures, we have created two different list classes: `ArrayList` and `LinkedList`. And these are also the topic of project 2, where you are getting more experience with them. These two classes both implement the *list* abstract data type (ADT), but with two different data structures as their basis:\n",
    "* The `ArrayList` is built on top of the dynamic array structure\n",
    "* The `LinkedList` is built on top of the linked list structure.\n",
    "\n",
    "We will now look closer at how the different choice for the basis data structure, affects their performance. To do so, we will go through the different operations of the list ADT, that both classes support, such as insertions, removal, and indexing.\n",
    "\n",
    "We will characterize the costs in terms of the *length* of the list, which we will denote $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appending\n",
    "\n",
    "Let us start with the operation of appending new elements to the end of the list. Both classes support this method, but what is the *cost* of appending?\n",
    "\n",
    "##### Dynamic Array\n",
    "\n",
    "For a dynamic array, appending an element is the same as assign that value to the first unused element of the underlying storage array. The cost of this operation is not dependent on the length of the list. It doesn't matter if the list has 100 or 100 million elements, appending a value is simply assigning a value to the next unused space. Thus, the cost of appending an element to our `ArrayList` is $\\mathcal{O}(1)$.\n",
    "\n",
    "However! Sometimes, there *won't* be any free space left in the underlying storage array. In this case we had to do a \"resize\" operation as we called it. To resize our array, we doubled the size of the underlying array, if the list has $n$ elements, this means we allocate an array of $2n$ elements. Then we copy over the $n$ elements of the array and deallocate the previous storage array. The bigger the list is, the more work it is to do the resizing. The resize operation thus scales as $\\mathcal{O}(n)$.\n",
    "\n",
    "Now, appending to our `ArrayList` only *sometimes* incurs the cost of a resize operation, so how can we include this in our analysis? We will return to this point shortly, for now we simply say that the cost of the `ArrayList.append` method is $\\mathcal{O}(1)$ + an occasional $\\mathcal{O}(n)$.\n",
    "\n",
    "##### Linked List\n",
    "\n",
    "Now we turn to the linked list. Recall that appending an element to the linked list required starting at the head of the list, and iterating through to the end of the list. The method looked something like this:\n",
    "```C++\n",
    "void append(int val) {\n",
    "    if (head == nullptr) {\n",
    "        head = new Node(val);\n",
    "        return;\n",
    "    }\n",
    "    \n",
    "    Node* current = head;\n",
    "    while (current->next != nullptr) {\n",
    "        current = current->next;\n",
    "    }\n",
    "    current->next = new Node(val);\n",
    "}\n",
    "```\n",
    "Because we are iterating from the head of the list, all the way to the end, we are doing $n$ operations. If the list consists of 100 nodes, the loop needs to run 100 times. Once we get to the end, actually attaching the new node is only a few operations. This means that most of the work of appending to the `LinkedList` is iterating through to the end, but because of this operation, the whole append method costs $\\mathcal{O}(n)$.\n",
    "\n",
    "We see that because we need to iterate from the start, as the list grows, the cost of appending more elements also grow. If the size of the array itself doubles, then the cost of the array will double.\n",
    "\n",
    "##### Comparing the two:\n",
    "\n",
    "We have seen that appending costs $\\mathcal{O}(1)$ for the `DynamicArray` and $\\mathcal{O}(n)$ for the `LinkedList`, this means that while the two might be equally fast for appending a few elements, as the lists grow, appending more elements to the linked list will become slower and slower, while for the `DynamicArray` they will stay the same.\n",
    "\n",
    "Now, you might be questioning this comparison as unfair, because the dynamic array also needed to resize every now and then, a costly operation. And that is a good point, so let us look a bit closer at that. Instead of referring to a single append operation, which most likely won't but could possibly incur a resize. Let us look at a whole set of appends, which definitely will incur a resize. We can then find the \"average\" cost of an append. This concept of \"average cost\" is called the *amortized* cost of an operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Amortized Cost\n",
    "\n",
    "The *amortized* cost of an operation is the averaged cost of a single operation, if one carries a lot of them. The term itself might be new and cryptic to you, but it comes from the world of finance and economics. A big buisness might *amortize* their costs by spreading them evenly throughout the year for example. It is simply a way of averaging costs and payments.\n",
    "\n",
    "Instead of analyzing a single append. Let us say we start with an empty dynamic array, and append $n$ elements to it:\n",
    "```C++\n",
    "ArrayList example;\n",
    "\n",
    "for (int i=0; i<n; i++) {\n",
    "    example.append(i);\n",
    "}\n",
    "```\n",
    "What is the total cost of this operation, in big Oh, as a function $n$? Well, the $n$ appends themselves each cost $\\mathcal{O}(1)$, and so $n$ operations of $\\mathcal{O}(1)$ will be a total of $\\mathcal{O}(n)$.\n",
    "\n",
    "But what is the total cost of all the resized required to reach $n$? To understand this, it is easiest to start from the end, and work back to the start. The final resize neeeded was to increase the underlying storage from $n/2$ to $n$, which would cost $n$. The resize before that would need to take it from $n/4$ to $n/2$, which would cost $n/2$. The one before that would need to take it from $n/8$ to $n/4$, which would cost $n/4$ and so on. So the total cost of all the resizes would be:\n",
    "$$n + \\frac{n}{2} + \\frac{n}{4} + \\frac{n}{8} + \\ldots,$$\n",
    "all the way down to the empty list. The exact number of resize operations will depend on what $n$ is.\n",
    "\n",
    "This sum is a geometric progression, which sums out to $2n$. The easiest way to see this is simply to draw out the series as a bunch of boxes\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/4b/Geometric_progression_convergence_diagram.svg/525px-Geometric_progression_convergence_diagram.svg.png\" width=400>\n",
    "So while each resize might be costly, we carry out so few of them, that the *total cost* of all the resizes also becomes $\\mathcal{O(n)}$\n",
    "\n",
    "\n",
    "So the total cost of appending $n$ elements to an empty `DynamicArray` is\n",
    "\n",
    "$$\\mbox{(cost of $n$ appends)} = \\mathcal{O}(n) + \\mathcal{O}(n) = \\mathcal{O}(n).$$\n",
    "\n",
    "But if the cost of doing $n$ appends has a total cost of $\\mathcal{O}(n)$, then the average/amortized cost of a single operations must be\n",
    "\n",
    "$$\\mbox{(amortized cost of 1 append)} = \\frac{1}{n}\\cdot \\mbox{(cost of $n$ appends)} = \\frac{1}{n}\\cdot \\mathcal{O}(n) = \\mathcal{O}(1).$$\n",
    "\n",
    "So on *average*, the cost of appending an element to our `ArrayList` is still not dependent on the size of that list, unlike for the linked lists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inserting elements to the front\n",
    "\n",
    "We have now look at the cost of appending elements to the back of the list, but what about pushing them to the front? For our `LinkedList` we implemented this as a `push_front` method, or alternatively as the `insert` method where we specify the index 0. We did not implement this method for the `ArrayList` in the lecture, but that is part of project 2. While we have not implemented it, we will still try to analyze what it would cost.\n",
    "\n",
    "#### Dynamic Array\n",
    "\n",
    "To insert an element at the front of a dynamic array, we would need to first make space for it, by moving every other element in the list one index up. This operation would require the existing $n$ elements, which would cost $\\mathcal{O}(n)$. After this is done, adding the new element would be simple, only requiring a single assignment, which would be $\\mathcal{O}(1)$. So in total, appending to the front of a dynamic array is\n",
    "$$\\mathcal{O}(n) + \\mathcal{O}(1) = \\mathcal{O}(n).$$\n",
    "\n",
    "#### Linked List\n",
    "\n",
    "For the linked list, we did implement the `push_front` method, and to do so was almost trivial because of the `head` pointer of the class pointing at the first element:\n",
    "```C++\n",
    "void push_front(val) {\n",
    "    head = new Node(val, head);\n",
    "}\n",
    "```\n",
    "This method is just as simple if there is 0 elements in the list, or 10, or a million. No matter the size of the list it is only a few operations, and so the cost is $\\mathcal{O}(1)$.\n",
    "\n",
    "#### Comparing the two\n",
    "\n",
    "First we saw that the cost of appending is constant for a dynamic array: $\\mathcal{O}(1)$, and scales linearly with the linked list: $\\mathcal{O}(n)$. And now we have seen that the reverse is true for adding elements to the front, it is constant for the linked list, and linear for the array list!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a Tail Pointer to the Linked List\n",
    "\n",
    "In our analysis of the linked list data structures so far, we have seen that adding an element to the front of the linked list is easier, meaning it costs $\\mathcal{O}(1)$, than than adding it to the back, where the cost is $\\mathcal{O}(n)$. This is simply because we have the `head` pointer to the first element, but to get to the back of the list, we need to iterate through the whole chain.\n",
    "\n",
    "However, there is no reason we cannot also add a pointer to the end of the chain, which we can use as a shortcut to get there. This pointer is often called the `tail` pointer, because it points to the tail of the list. If we have a tail pointer in our class, then we can rewrite the append method as\n",
    "```C++\n",
    "void append(int val) {\n",
    "    if (head == nullptr) {\n",
    "        head = new Node(val);\n",
    "        tail = head;\n",
    "        return;\n",
    "    }\n",
    "\n",
    "    tail->next = new Node(val);\n",
    "    tail = tail->next;\n",
    "}\n",
    "```\n",
    "Which is practically just as easy as the `push_front` method. By adding the tail pointer, we have thus improved the scaling of our append method to $\\mathcal{O}(1)$, the same as for the `DynamicArray`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inserting into the middle\n",
    "\n",
    "So far we have covered inserting into the back, or to the front, of the list. But what about inserting somewhere in the middle? Here, we do not necessarily mean exactly the middle, but inserting at a given index $i$.\n",
    "\n",
    "\n",
    "#### Dynamic Array\n",
    "\n",
    "To insert somewhere in the middle of the list, the situation is slightly better than adding to the front, as we only have to move all the elements with indices above it to make room. However, inserting into the middle of the list would still mean moving something like half the elements of the list, i.e., $n/2$ elements. And so the cost would still increase linearily with $n$, so the cost would still be $\\mathcal{O}(n)$.\n",
    "\n",
    "If we instead want to give the cost as a function of both $n$ and $i$, then the cost would be:\n",
    "$$\\mathcal{O}(n-i),$$\n",
    "which shows that the further back in the list we want to insert, the bigger the index $i$, and the less the operation costs.\n",
    "\n",
    "#### Linked List\n",
    "\n",
    "For a linked list, the insertion of a new node itself is not very costly, we only need to create a new node and hook it into the chain. The insertion itself would only cost $\\mathcal{O}(1)$. However, just like appending (before we added the tail pointer), we first need to get to the right nodes to actually carry out this operation, and so this would cost $\\mathcal{O}(n)$, as we would need to start at the front of the list and iterate to whereever the node was to be connected.\n",
    "\n",
    "You might think we can fix this situation by having more reference pointer, just like the tail. But to insert like this at any given index $i$ is not feasible, as we would literally need to make another list to store all those reference pointers.\n",
    "\n",
    "However, as the cost of inserting itself is $\\mathcal{O}(1)$, we might be able to piggyback of other algorithms and methods that happen to be iterating through the list anyway. For a dynamic array, this won't be possible, because regardless of how you find the element, the cost comes from having to move all the other elements to make room.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizing so far\n",
    "\n",
    "| Operation         | Dynamic Array      | Linked List   | Linked list (w/ tail ref) |\n",
    "| ----------------- | --------------------- | ---------------- | -------------------- |\n",
    "| Insert at back\t| $\\mathcal{O}(1)^*$ | $\\mathcal{O}(n)$| $\\mathcal{O}(1)$  |\n",
    "| Insert at front\t| $\\mathcal{O}(n)$ | $\\mathcal{O}(1)$ | $\\mathcal{O}(1)$  |\n",
    "| Insert in middle\t| $\\mathcal{O}(n)$ | $\\mathcal{O}(n)$ | $\\mathcal{O}(n)$ | \n",
    "\n",
    "\\*) This is the amortized cost, i.e., the cost averaged over many operations\n",
    "\n",
    "To summarize, we see that the cost of inserting elements into the lists are different, depending on what data structure we use. From the table, it might seem that using a linked list with a tail pointer might be the most efficient, and if we were looking at only insertions, this would indeed be the case. But lists are not only used for insertions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing\n",
    "\n",
    "An important characteristic of using lists is that we can use indexing to read and write to elements of the list. We implemented this operation for both our lists by overloading the square-bracket operator. But what is actually the cost of the operation?\n",
    "\n",
    "#### Dynamic Array\n",
    "\n",
    "We implemented our indexing operator something like this:\n",
    "```C++\n",
    "int& operator[](int index) {\n",
    "    return data[index];\n",
    "}\n",
    "```\n",
    "Because our data is actually stored in an underlying storage array (called `data` in this example), indexing our `ArrayList` is actually just an alias for indexing the underlying array.\n",
    "\n",
    "One of the strengths of arrays is that the data lies contiguously in memory, and as we have seen earlier, indexing an array is actually just fancy memory address operations, so if we for example write `x[10]` to get the 10'th element of some array, then `x` is actually just a pointer/memory address, and the indexing operations means `*(x + 10)`, meaning we do some pointer arithmetic and then look up the value at the right memory address. \n",
    "\n",
    "If you don't recall all of these details from L12, don't worry to much about it. The important point is this: Because arrays are contiguous in memory, accessing any element by index is trivial, because C++ will be able to compute its way to the correct memory address and look up the right value. This is true regardless of the size of the array. Therefore, accessing an element by index in an array is $\\mathcal{O(1)}$.\n",
    "\n",
    "Because our `ArrayList` is built on top of arrays, our indexing operation will also be $\\mathcal{O}(1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linked Lists\n",
    "\n",
    "For linked lists, the situation is different. The elements, i.e., nodes of a linked list are *not* stored contiguously in memory. Each node can for all intents and purposes exist somewhere completely different in memory, the important thing is that each node knows where the next one is stored.\n",
    "\n",
    "However, this means that to get to an element based on its index $i$, we have to start at the front of the list, and iterate all the way to the right element. We implemented this as something like\n",
    "```C++\n",
    "int& operator[] (int index) {\n",
    "    Node* current = head;\n",
    "    \n",
    "    for (int i=0; i<index; i++) {\n",
    "        current = current->next;\n",
    "    }\n",
    "    return current;\n",
    "}\n",
    "```\n",
    "So we start at the head, and move $i$ steps, where $i$ is the index. This means accesing an element by index costs $\\mathcal{O}(i)$ in terms of the index. Often we do not express costs in terms of indices, so we simply say that this costs $\\mathcal{O}(n)$. The bigger the list, the bigger the indicies we typically access, the more costly it will be.\n",
    "\n",
    "Indexing the final element in the list for example, would require us to iterate through the entire list. If we have a tail reference, getting to the last element is easy, but getting to the second-to-last element is still hard, because while we have a tail reference, we have no way to iterate backwards in our linked list. If we had implemented a *doubly* linked list, we could iterate in from either side of the array. This would improve things somewhat, but for most indicies, we would still need considerable iterating to get to a given index.\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing the two\n",
    "\n",
    "For arrays and dynamic arrays, indexing is a \"free\" operation. It does not matter what index we want to access either, they are all equal. This is often referred to as *random access*. The name implies that it doesn't matter what order we access the elements of the list in, it might as well be random. A better name for it is *direct access*, we can directly go in and access any element by index. You might be familiar with the term *random access memory* (RAM), which refers to the normal memory on the computer, this is also called random access because accessing any part of it should take roughly the same amount of time.\n",
    "\n",
    "As we have seen, a linked list is not a direct access data structure. We cannot go into any given index directly, but have to iterate through the sequence from the start. This is known as *sequential access*.\n",
    "\n",
    "<img src=https://upload.wikimedia.org/wikipedia/commons/thumb/a/a7/Random_vs_sequential_access.svg/600px-Random_vs_sequential_access.svg.png width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizing\n",
    "\n",
    "Our findings are summarized in the table below. By analyzing the costs of the different operations, we can see how the data structures affect the overlying data type. While both data structures can support the same list ADT, the performance will differ.\n",
    "\n",
    "| Operation         | Array |  Dynamic Array      | Linked List   | Linked list (w/ tail ref) |\n",
    "| ----------------- | ------ | --------------------- | ---------------- | -------------------- |\n",
    "| Insert at back\t| - | $\\mathcal{O}(1)^*$ | $\\mathcal{O}(n)$| $\\mathcal{O}(1)$  |\n",
    "| Insert at front\t| - | $\\mathcal{O}(n)$ | $\\mathcal{O}(1)$ | $\\mathcal{O}(1)$  |\n",
    "| Insert in middle\t| - | $\\mathcal{O}(n)$ | $\\mathcal{O}(n)$ | $\\mathcal{O}(n)$ | \n",
    "| Get element by index | $\\mathcal{O}(1)$ | $\\mathcal{O}(1)$ | $\\mathcal{O}(n)$ | $\\mathcal{O}(n)$\n",
    "\n",
    "\\*) This is the amortized cost, i.e., the cost averaged over many operations\n",
    "\n",
    "### What we haven't analyzed\n",
    "\n",
    "Our analysis hasn't been too extensive, mostly due to time, we have simply compared some of the most important operations in term of big Oh. But there are other important differences between the data structures we haven't covered, such as the difference in memory usage of the two. Or other operations such as removing elements. It also turns out that an important difference in practice is that arrays are often fast, because when elements are contiguous in memory, they can be loaded into the CPU cache *faster*. Facts such as these are hard to include into our algorithm analysis.\n",
    "\n",
    "While our analysis is simplified and theoretical, it is still very useful, and analysis such as the ones we have carried out are an important part for algorithms and data structures.\n",
    "\n",
    "\n",
    "### Final recommendation\n",
    "\n",
    "For scientific computing, dynamic arrays usually win out on efficiency, because they are stored contiguously in memory. In practice therefore, you might rarely use, or need to use, linked lists.\n",
    "\n",
    "However, knowing about linked lists, and how to implement and analyze them is still a valuable skill, as they are an important introductory data structure and something you might be expected to know about if you every move further into computational science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Analogies\n",
    "\n",
    "The major differences between a linked list and a array list is how elements are added and removed from the list, and how they are read. This can be a very abstract concept, so many people like to make some analogies to understand and remember the differences.\n",
    "\n",
    "For example, when it comes to indexing, an example of a linked list would be the alphabet. Most people remember the alphabet as a linked list. You can illustrate this, because few people can answer the question *Which letter is the 17th in the alphabet?* directly, i.e., most people cannot \"index\" the alphabet. Instead they have to start at the beginning, and count their way to the 17th letter/element. However, ask someone \"which letter comes after P in the alphabet\" and most people probably wouldn't even have to think before answering.\n",
    "\n",
    "Indexing an array however, can be thought of like the pages in a book. If someone asks you to open a book to page number 277, you wouldn't have to start on page 1 and flip each page to get to the right spot. Instead, you could simply go directly to the right page.\n",
    "\n",
    "Similarily, we can talk about adding/removing elements. An analogy for adding elements to a dynamic array could be a stack of books lying on a table. Adding a book to the end of the stack is very easy, you just place it on top, where there is room. However, if you want to insert the book into the bottom or middle of the pile, you would have to do a lot more work. The bigger the stack, the more work.\n",
    "\n",
    "<img src=\"fig/stack_of_books.jpg\" width=100>\n",
    "\n",
    "Adding or removing elements from a linked list can be though of like modifying a chain, such as the one for your bicycle. If you need to lengthen your bike chain, you simply disconnect two of the \"nodes\" of the chain, add inn some more, and click them back together. It doesn't matter how long the chain is, adding more chains is just as hard. For our \"insert into the middle\" we also added the \"search time\" to get to the right node. This would be like a bike chain with a broken \"node\", first you would need to find the broken linker, to do this you first \"iterate\" through the chain, then you pop the broken one out and a fresh one in.\n",
    "\n",
    "<img src=\"fig/bike_chain.jpg\" width=300>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting Algorithms\n",
    "\n",
    "For the rest of this lecture, we will talk about *sorting algorithms*. Or more specificially, *comparison sorts*. Comparison sorts are sorting algorithms that work by sorting a sequence of items (letters, numbers, objects) in which the order must be worked out by comparing objects. For example, sorting a list of numbers from smallest to biggest is a comparison sort, because to find the bigger of two numbers, we compare them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bubble Sort\n",
    "\n",
    "One of the simplest sorting algorithms, which we have also covered earlier in this course, is the *bubble sort* algorithm. Let us look at one implementation.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/c/c8/Bubble-sort-example-300px.gif\" width=400>\n",
    "<center>A bubble sort gif linked from Wikipedia.</center>\n",
    "\n",
    "First we define a swap-metod, to easily swap two elements in the sequence:\n",
    "```C++\n",
    "void swap(int &a, int &b) {\n",
    "    int temp = a;\n",
    "    a = b;\n",
    "    b = temp;\n",
    "}\n",
    "```\n",
    "Recall that this works because we are doing a call by reference, using `&`.\n",
    "\n",
    "Now we implement the bubble sort like an in-place sorting. If we are sorting in-place, we need to do a call-by-reference.\n",
    "```C++\n",
    "void bubble_sort(vector<int> &input) {\n",
    "\tfor (int end=input.size(); end>0; end--) {\n",
    "\t\tfor (int i=0; i<end-1; i++) {\n",
    "\t\t\tif (input[i] > input[i+1]) {\n",
    "\t\t\t\tswap(input[i], input[i+1]);\n",
    "\t\t\t}\n",
    "\t\t}\n",
    "\t}\n",
    "}\n",
    "```\n",
    "\n",
    "Let us analyze the cost of this algorithm. We first iterate through the whole list, then the biggest element will be last, so we can now iterate through the whole list except the last element, and then the whole list except the two last, etc. For each iteration we perform at least one comparison, and perhaps a swap, let us say for each iteration, we do $c$ operations. This we get the sum:\n",
    "\n",
    "$$(n-1)c + (n-2)c + (n-3)c + \\ldots + 2c + c.$$\n",
    "\n",
    "This is essentially a triangle number, which turns out to give the sum\n",
    "$$\\frac{n(n-1)}{2}c$$\n",
    "\n",
    "As before, the coeficients and everything is unimportant, so we can summarize this as saying bubble sort takes $\\mathcal{O}(n^2)$ operations. This means, that if we double the length of the input list, bubble sort will use roughly four times longer to sort the list.\n",
    "\n",
    "Note that the algorithm is $\\mathcal{O}(n^2)$ in the worst-case, and the best-case. Because even if we are not doing any swaps, we are still performing $\\mathcal{O}(n^2)$ comparisons. This means that even if we send in an *already sorted list* to bubble sort, it will spend $\\mathcal{O}(n^2)$ \"sorting\" it. That is sort of a waste, but we can improve this.\n",
    "\n",
    "### An improved bubble sort\n",
    "\n",
    "To improve our algorithm, we simply abort our algorithm if we have iterated through the whole list without making any swaps. We can do this by adding a boolean flag:\n",
    "```C++\n",
    "void bubble_sort(vector<int> &input) {\n",
    "    bool swapped;\n",
    "\tfor (int end=input.size(); end>0; end--) {\n",
    "        swapped = false;\n",
    "\t\tfor (int i=0; i<end-1; i++) {\n",
    "\t\t\tif (input[i] > input[i+1]) {\n",
    "\t\t\t\tswap(input[i], input[i+1]);\n",
    "                swapped = true;\n",
    "\t\t\t}\n",
    "\t\t}\n",
    "        if (swapped == false) {\n",
    "            return;\n",
    "        }\n",
    "\t}\n",
    "}\n",
    "```\n",
    "\n",
    "In the worst case scenario, we still need to carry out all the same iterations as before, so the worst-case hasn't gotten any better with our new algorithm. It hasen't gotten any worse either, it simply adds a few constant steps into the algorithm, but that only changes the coefficients, which we don't care about.\n",
    "\n",
    "The best-case scenario however, has improved. If we now send in a sorted list, the algorithm iterates through it once, doing $(n-1)$ comparisons. No elements are swapped, so the algorithm terminates. I.e., the best case is $\\mathcal{O}(n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection Sort\n",
    "\n",
    "A different sorting algorithm, which we haven't covered in this course, but which is perhaps even simpler than bubble sort, is *selection sort*. The algorithm of selection sort is fairly easy:\n",
    "* Find the smallest element in the list that hasn't been moved yet\n",
    "* Put it in into its correct index\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/b/b0/Selection_sort_animation.gif\" width=300>\n",
    "<center>A selection sort gif linked from Wikipedia.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A possible implementation of selection sort is as follows:\n",
    "\n",
    "```C++\n",
    "void selection_sort(vector<int> &input) {\n",
    "\tint smallest; // stores index, not value\n",
    "\n",
    "\tfor (int i=0; i<input.size(); i++) {\n",
    "\t\t// Find smallest element\n",
    "\t\tsmallest = i;\n",
    "\t\tfor (int j=i+1; j<input.size(); j++) {\n",
    "\t\t\tif (input[j] < input[smallest]) {\n",
    "\t\t\t\tsmallest = j;\n",
    "\t\t\t}\n",
    "\t\t}\n",
    "\n",
    "\t\t// Place into its correct spot\n",
    "\t\tswap(input[i], input[smallest]);\n",
    "\t}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Cost of Selection sort\n",
    "\n",
    "This sorting algorithm consists of two loops. The outer loop iterates over each element in the list, and so repeats $n$ times. For each iteration, we loop over the remaining $n-i$ elements of the list, meaning we again get a triangle number sum:\n",
    "$$n + (n-1) + (n-2) + \\ldots + 1 = \\frac{n(n+1)}{2}.$$\n",
    "At this point, you might get tired of doing these sums, which is fair. In fact, in cases such as this, doing \"$n-i$\" can simply be assumed to be $\\mathcal{O}(n)$.\n",
    "\n",
    "So we do to nested loops, the outer of $n$ iterations, and the inner of roughly $n$ iterations, meaning the total cost is\n",
    "$$\\mathcal{O}(n^2).$$\n",
    "As for our first bubble sort implementation, this is both the best case and the worst case of the algorithm. However, in this case we cannot simply add the \"swap\" flag solution, because there is no way to build this into the algorithm. We could of course simply add the step: \"Check to see if the list is sorted\" first, but this would be to add a step that has nothing to do with the algorithm, which is quite pointless. This is unlike bubble sort case, because in that example we were already comparing all elements to begin with, i.e. the added cost was $\\mathcal{O}(1)$, not $\\mathcal{O}(n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are Bubble Sort and Selection Sort \"good\" sorting algorithms?\n",
    "\n",
    "While both of these algorithms are easy to implement, and require little code, they are, as we have both seen, $\\mathcal{O}(n^2)$, meaning they scale poorly with the size of the input.\n",
    "\n",
    "Is it possible to do better? Yes, it is. In fact, we know how much better, because it has been shown that the lower limit for both the average case and the worst case of comparison sorting is $\\mathcal{O}(n\\log n)$, which is much better than $\\mathcal{O}(n^2)$. To see why, remember that $n^2 = n\\cdot n$. In comparison, $n \\cdot \\log n$ is multiplying by $\\log n$ instead of $n$, which is much smaller. The growth of a $\\mathcal{O}(n \\log n)$ is therefore close to linear, which is why we call it *loglinear growth*. The best case has a lower limit of $\\mathcal{O}(n)$. Because, to know if a list is sorted, we need to carry out $n-1$ comparisons, which must be $\\mathcal{O}(n)$.\n",
    "\n",
    "Have any practical algorithm reached this theoretical limit? Yes indeed, most sorting algorithms that are used are $\\mathcal{O}(n\\log n)$ in both the average and the worst case. In fact, there are so many to choose from, that it is a bit staggering. Wikipedia has a nice article on [sorting algorithms](https://en.wikipedia.org/wiki/Sorting_algorithm#Comparison_of_algorithms) that has a summary comparing many of the most well-know algorithms and their computational costs.\n",
    "\n",
    "We won't go through and show many of these sorting algorithms and how they work now, as we are out of time. But taking the time to look through the table on Wikipedia is a good idea, to familiarize yourself with some of the different options. One of the most popular algorithms, *quicksort*, is actually $\\mathcal{O}(n^2)$, but it has been shown that it is $\\mathcal{O}(n\\log n)$ on most realistic input, and so it's average cost is $\\mathcal{O}(n\\log n)$ and it will often be the fastest in practice. Other popular choices of good and fast sorting algorithms are *mergesort* and *heapsort*.\n",
    "\n",
    "At this point, an interesting question might be what sorting algorithm the build in Python `sort` uses. The answer is [Timsort](https://en.wikipedia.org/wiki/Timsort), this is a special sorting algorithm made for Python in 2002. It is a hybrid between *mergesort* and *insertion sort*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Sorting Algorithms\n",
    "\n",
    "As there are so many sorting algorithms out there, an interesting way to compare them could be to visualize how they sort their lists. This can be done in many ways, for example like shown in the Youtube video below, which shows of 15 different sorting algorithms on lists of various sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/kPRA0W1kECg\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "# Embedd Youtube Video\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/kPRA0W1kECg\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting Folk Dances\n",
    "\n",
    "Or you can be inspired by the Romian Sapientia University, which has created and shared a whole list of sorting algorithms explained through traditional folk dances. These are shared through the Youtube-channel [AlgoRythmics](https://www.youtube.com/user/AlgoRythmics/videos), which is well worth a watch.\n",
    "\n",
    "See for example their video of *insert-sort* below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/ROalU379l3U\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "# Embedd Youtube Video\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/ROalU379l3U\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}